import os
import requests
import streamlit as st
from openai import OpenAI
from src.config import DASHSCOPE_BASE_URL, ZIWEI_API_URL_DEFAULT

@st.cache_data(ttl=3600)  # ç¼“å­˜ 1 å°æ—¶
def get_ziwei_data(birthday, hour_index, gender, target_year, is_lunar=False, is_leap=False):
    api_url = os.getenv('ZIWEI_API_URL', ZIWEI_API_URL_DEFAULT)
    
    try:
        payload = {
            'birthday': birthday,
            'hourIndex': int(hour_index),
            'gender': gender,
            'isLunar': is_lunar,
            'isLeap': is_leap,
            'targetYear': target_year
        }
        
        response = requests.post(api_url, json=payload, timeout=10)
        response.raise_for_status()
        
        return response.json()
    except requests.exceptions.ConnectionError:
        st.error("æ— æ³•è¿æ¥åˆ°ç´«å¾®æ–—æ•°è®¡ç®—æœåŠ¡ã€‚è¯·ç¡®ä¿ Node.js æœåŠ¡æ­£åœ¨è¿è¡Œåœ¨ http://localhost:3000")
        st.info("å¯åŠ¨å‘½ä»¤: node src/server.js")
        return None
    except requests.exceptions.Timeout:
        st.error("è¯·æ±‚è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•")
        return None
    except requests.exceptions.RequestException as e:
        st.error(f"è¯·æ±‚å¤±è´¥: {e}")
        return None
    except Exception as e:
        st.error(f"è®¡ç®—å¤±è´¥: {e}")
        import traceback
        st.error(traceback.format_exc())
        return None

def get_llm_response(messages):
    api_key = os.getenv("DASHSCOPE_API_KEY")
    if not api_key:
        st.warning("ğŸ’¡ æç¤ºï¼šAIæœåŠ¡æš‚ä¸å¯ç”¨ï¼Œè¯·å…ˆåˆ°'å‘½ç›˜æ˜¾ç¤º'é¡µé¢æ’ç›˜ï¼Œæˆ–åˆ·æ–°é¡µé¢é‡è¯•")
        return None
    
    model = st.session_state.get('selected_model', 'qwen3-max')
    base_url = DASHSCOPE_BASE_URL
    
    client = OpenAI(base_url=base_url, api_key=api_key)
    try:
        return client.chat.completions.create(model=model, messages=messages, stream=True, temperature=0.7)
    except Exception as e:
        st.error(f"AIè°ƒç”¨å¤±è´¥: {e}")
        return None
